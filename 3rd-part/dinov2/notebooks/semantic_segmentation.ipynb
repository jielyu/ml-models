{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c7cd430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Meta Platforms, Inc. and affiliates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23886c8",
   "metadata": {},
   "source": [
    "# Semantic Segmentation <a target=\"_blank\" href=\"https://colab.research.google.com/github/facebookresearch/dinov2/blob/main/notebooks/semantic_segmentation.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7485d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "INSTALL = False # Switch this to install dependencies\n",
    "if INSTALL: # Try installing package with extras\n",
    "    REPO_URL = \"https://github.com/facebookresearch/dinov2\"\n",
    "    !{sys.executable} -m pip install -e {REPO_URL}'[extras]' --extra-index-url https://download.pytorch.org/whl/cu117  --extra-index-url https://pypi.nvidia.com\n",
    "else:\n",
    "    REPO_PATH = \"/home/jielyu/CodeStudio/github/dinov2\" # Specify a local path to the repository (or use installed package instead)\n",
    "    sys.path.append(REPO_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10896f2",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e311c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import itertools\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from mmseg.apis import init_segmentor, inference_segmentor\n",
    "\n",
    "import dinov2.eval.segmentation.models\n",
    "\n",
    "\n",
    "class CenterPadding(torch.nn.Module):\n",
    "    def __init__(self, multiple):\n",
    "        super().__init__()\n",
    "        self.multiple = multiple\n",
    "\n",
    "    def _get_pad(self, size):\n",
    "        new_size = math.ceil(size / self.multiple) * self.multiple\n",
    "        pad_size = new_size - size\n",
    "        pad_size_left = pad_size // 2\n",
    "        pad_size_right = pad_size - pad_size_left\n",
    "        return pad_size_left, pad_size_right\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def forward(self, x):\n",
    "        pads = list(itertools.chain.from_iterable(self._get_pad(m) for m in x.shape[:1:-1]))\n",
    "        output = F.pad(x, pads)\n",
    "        return output\n",
    "\n",
    "\n",
    "def create_segmenter(cfg, backbone_model):\n",
    "    model = init_segmentor(cfg)\n",
    "    model.backbone.forward = partial(\n",
    "        backbone_model.get_intermediate_layers,\n",
    "        n=cfg.model.backbone.out_indices,\n",
    "        reshape=True,\n",
    "    )\n",
    "    if hasattr(backbone_model, \"patch_size\"):\n",
    "        model.backbone.register_forward_pre_hook(lambda _, x: CenterPadding(backbone_model.patch_size)(x[0]))\n",
    "    model.init_weights()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b734e5",
   "metadata": {},
   "source": [
    "## Load pretrained backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241da39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "BACKBONE_SIZE = \"small\" # in (\"small\", \"base\", \"large\" or \"giant\")\n",
    "\n",
    "\n",
    "backbone_archs = {\n",
    "    \"small\": \"vits14\",\n",
    "    \"base\": \"vitb14\",\n",
    "    \"large\": \"vitl14\",\n",
    "    \"giant\": \"vitg14\",\n",
    "}\n",
    "backbone_arch = backbone_archs[BACKBONE_SIZE]\n",
    "backbone_name = f\"dinov2_{backbone_arch}\"\n",
    "\n",
    "backbone_model = torch.hub.load(repo_or_dir=\"facebookresearch/dinov2\", model=backbone_name)\n",
    "backbone_model.eval()\n",
    "backbone_model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e84a79c",
   "metadata": {},
   "source": [
    "## Load pretrained segmentation head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198e3c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "\n",
    "import mmcv\n",
    "from mmcv.runner import load_checkpoint\n",
    "\n",
    "\n",
    "def load_config_from_url(url: str) -> str:\n",
    "    with urllib.request.urlopen(url) as f:\n",
    "        return f.read().decode()\n",
    "\n",
    "\n",
    "HEAD_SCALE_COUNT = 3 # more scales: slower but better results, in (1,2,3,4,5)\n",
    "HEAD_DATASET = \"voc2012\" # in (\"ade20k\", \"voc2012\")\n",
    "HEAD_TYPE = \"ms\" # in (\"ms, \"linear\")\n",
    "\n",
    "\n",
    "DINOV2_BASE_URL = \"https://dl.fbaipublicfiles.com/dinov2\"\n",
    "head_config_url = f\"{DINOV2_BASE_URL}/{backbone_name}/{backbone_name}_{HEAD_DATASET}_{HEAD_TYPE}_config.py\"\n",
    "head_checkpoint_url = f\"{DINOV2_BASE_URL}/{backbone_name}/{backbone_name}_{HEAD_DATASET}_{HEAD_TYPE}_head.pth\"\n",
    "\n",
    "cfg_str = load_config_from_url(head_config_url)\n",
    "cfg = mmcv.Config.fromstring(cfg_str, file_format=\".py\")\n",
    "if HEAD_TYPE == \"ms\":\n",
    "    cfg.data.test.pipeline[1][\"img_ratios\"] = cfg.data.test.pipeline[1][\"img_ratios\"][:HEAD_SCALE_COUNT]\n",
    "    print(\"scales:\", cfg.data.test.pipeline[1][\"img_ratios\"])\n",
    "\n",
    "model = create_segmenter(cfg, backbone_model=backbone_model)\n",
    "load_checkpoint(model, head_checkpoint_url, map_location=\"cpu\")\n",
    "model.cuda()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8169917",
   "metadata": {},
   "source": [
    "## Load sample image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d74eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def load_image_from_url(url: str) -> Image:\n",
    "    with urllib.request.urlopen(url) as f:\n",
    "        return Image.open(f).convert(\"RGB\")\n",
    "\n",
    "\n",
    "EXAMPLE_IMAGE_URL = \"https://dl.fbaipublicfiles.com/dinov2/images/example.jpg\"\n",
    "\n",
    "\n",
    "image = load_image_from_url(EXAMPLE_IMAGE_URL)\n",
    "display(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59473594",
   "metadata": {},
   "source": [
    "## Semantic segmentation on sample image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32edb238",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import dinov2.eval.segmentation.utils.colormaps as colormaps\n",
    "\n",
    "\n",
    "DATASET_COLORMAPS = {\n",
    "    \"ade20k\": colormaps.ADE20K_COLORMAP,\n",
    "    \"voc2012\": colormaps.VOC2012_COLORMAP,\n",
    "}\n",
    "\n",
    "\n",
    "def render_segmentation(segmentation_logits, dataset):\n",
    "    colormap = DATASET_COLORMAPS[dataset]\n",
    "    colormap_array = np.array(colormap, dtype=np.uint8)\n",
    "    segmentation_values = colormap_array[segmentation_logits + 1]\n",
    "    return Image.fromarray(segmentation_values)\n",
    "\n",
    "\n",
    "array = np.array(image)[:, :, ::-1] # BGR\n",
    "segmentation_logits = inference_segmentor(model, array)[0]\n",
    "segmented_image = render_segmentation(segmentation_logits, HEAD_DATASET)\n",
    "display(segmented_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c837ad9",
   "metadata": {},
   "source": [
    "## Load pretrained segmentation model (Mask2Former)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ad8b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dinov2.eval.segmentation_m2f.models.segmentors\n",
    "\n",
    "CONFIG_URL = f\"{DINOV2_BASE_URL}/dinov2_vitg14/dinov2_vitg14_ade20k_m2f_config.py\"\n",
    "CHECKPOINT_URL = f\"{DINOV2_BASE_URL}/dinov2_vitg14/dinov2_vitg14_ade20k_m2f.pth\"\n",
    "\n",
    "cfg_str = load_config_from_url(CONFIG_URL)\n",
    "cfg = mmcv.Config.fromstring(cfg_str, file_format=\".py\")\n",
    "\n",
    "model = init_segmentor(cfg)\n",
    "load_checkpoint(model, CHECKPOINT_URL, map_location=\"cpu\")\n",
    "model.cuda()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca63227",
   "metadata": {},
   "source": [
    "## Semantic segmentation on sample image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922983d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "array = np.array(image)[:, :, ::-1] # BGR\n",
    "segmentation_logits = inference_segmentor(model, array)[0]\n",
    "segmented_image = render_segmentation(segmentation_logits, \"ade20k\")\n",
    "display(segmented_image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
